<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents</title>

  <!-- TODO: Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Alex Irpan<sup>1</sup>,</span>
            <span class="author-block">
              Keerthana Gopalakrishnan<sup>1</sup>,</span>
            <span class="author-block">
              Sergey Levine<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Ted Xiao<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Peng Xu<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Ryan Julian<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Sean Kirmani<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Debidatta Dwibedi<sup>1</sup>,</span>
            <span class="author-block">
              Karol Hausman<sup>1</sup>,</span>
            <span class="author-block">
              Dorsa Sadigh<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Brian Ichter<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Yao Lu<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Stefan Welker<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Pannag Sanketi<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Kanishka Rao<sup>1</sup>,</span>
            <span class="author-block">
              Edward Lee<sup>1</sup>,</span>
            <span class="author-block">
              Fei Xia<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Isabel Leal<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Pierre Sermanet<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Nikhil Joshi<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Zhuo Xu<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Quan Vuong<sup>1</sup>,</span>
            <span class="author-block">
              Michael Ahn<sup>1</sup>,</span>
            <span class="author-block">
              Chelsea Finn<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Montse Gonzalez Arenas<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Steve Xu<sup>1</sup>,</span>
            </span>
            <span class="author-block">
              Sharath Maddineni<sup>1</sup></span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such “in-the-wild” data collected by AutoRT is significantly more diverse, and that AutoRT’s use of LLMs allows for instruction following data collection robots that are aligned with human preferences.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            TODO: Write this.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
    <!-- Experiments. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            TODO: Write this.
          </p>
        </div>
      </div>
    </div>
    <!--/ Experiments. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gdm2024autort,
  author    = {TODO},
  title     = {AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents},
  journal   = {ICRA},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/pdf/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This website is borrowed from <a href="https://nerfies.github.io/">nerfies</a>.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
